---
title: LLMs Technology
author: yuyao
date: 2024-04-24 23:55:16 +0800 
categories: [Trends, ai]
tags: [ai]
---

全链路：大模型设计、训练、微调、压缩、部署、托管、推理
优化点：推理加速（算子融合、KV缓存、多卡并行、量化压缩）、断点续传

## API

#### PyTorch：

[云原生的弹性 AI 训练系列之二：PyTorch 1.9.0 弹性分布式训练的设计与实现](https://www.cnblogs.com/tencent-cloud-native/p/15186403.html)

## Model Download

#### Hugging Face：

1.`git lfs` 下载到当前路径
```shell
git lfs install
git clone https://huggingface.co/Qwen/Qwen2-0.5B
```
2.调用本地没有的模型会自动联网下载，默认下载到 `~/.cache/huggingface/`，[`cache_dir`](https://github.com/huggingface/transformers/blob/e65502951593a76844e872fee9c56b805598538a/src/transformers/models/auto/tokenization_auto.py#L725) 指定下载路径
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B", cache_dir = ".")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-0.5B", cache_dir = ".")
```
3.[huggingface_hub](https://huggingface.co/docs/huggingface_hub/guides/download) 下载，默认下载到 `~/.cache/huggingface/`，`cache_dir` 或 `local_dir` 指定下载路径，[两者区别](https://huggingface.co/docs/huggingface_hub/guides/download#download-files-to-a-local-folder)

下载单个文件：

```python
from huggingface_hub import hf_hub_download
hf_hub_download(repo_id="Qwen/Qwen2-0.5B", filename="model.safetensors", local_dir = ".")
```

下载整个仓库：
```python
from huggingface_hub import snapshot_download
snapshot_download(repo_id="Qwen/Qwen2-0.5B", local_dir = ".")
```
4.[`huggingface-cli` 命令行下载](https://huggingface.co/docs/huggingface_hub/guides/download#download-from-the-cli)，[Guides](https://huggingface.co/docs/huggingface_hub/guides/cli#download-to-a-local-folder)

```shell
huggingface-cli download gpt2 config.json model.safetensors --local-dir .
```
(可选) `huggingface-cli` 登录
```shell
pip install -U "huggingface_hub[cli]"
huggingface-cli login --token $HUGGINGFACE_TOKEN --add-to-git-credential
```
#### [Modelscope](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD)：

1.`git lfs` 下载到当前路径

```shell
git lfs install
git clone https://www.modelscope.cn/qwen/Qwen-7B-Chat.git
```

2.命令行下载多个文件，默认下载到 `~/.cache/modelscope`，`cache_dir` 指定路径
```shell
modelscope download --model 'AI-ModelScope/gpt2' config.json model.safetensors --cache_dir '.'
```

3.SDK下载

```python
from modelscope import snapshot_download
model_dir = snapshot_download('qwen/Qwen-7B-Chat', local_dir = ".")
```

## Embedding

[<mark><font color="red" face="Times New Roman" size=5>FlagEmbedding</font></mark>](https://github.com/FlagOpen/FlagEmbedding)

[Embedding Projector](https://projector.tensorflow.org/)

[OpenAI API](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)

[embedding的原理及实践](https://qiankunli.github.io/2022/03/02/embedding.html)

[Vector Database](https://www.youtube.com/watch?v=uQcBwN1PEyI)：[chroma](https://github.com/chroma-core/chroma)，[milvus](https://github.com/milvus-io/milvus)，[faiss](https://github.com/facebookresearch/faiss)

## Compiler

[<mark><font color="red" face="Times New Roman" size=5>tvm</font></mark>](https://github.com/apache/tvm)

## Fine Tuning

[<mark><font color="red" face="Times New Roman" size=5>LoRA</font></mark>](https://github.com/microsoft/LoRA)

[<mark><font color="red" face="Times New Roman" size=5>QLoRA</font></mark>](https://github.com/artidoro/qlora)

[<mark><font color="red" face="Times New Roman" size=5>LLaMA Factory</font></mark>](https://github.com/hiyouga/LLaMA-Factory)

[<mark><font color="red" face="Times New Roman" size=5>PEFT</font></mark>](https://github.com/huggingface/peft)

[<mark><font color="red" face="Times New Roman" size=5>FastChat</font></mark>](https://github.com/lm-sys/FastChat)

[<mark><font color="red" face="Times New Roman" size=5>unsloth</font></mark>](https://github.com/unslothai/unsloth)

[<mark><font color="red" face="Times New Roman" size=5>Firefly</font></mark>](https://github.com/yangjianxin1/Firefly)

[<mark><font color="red" face="Times New Roman" size=5>SWIFT</font></mark>](https://github.com/modelscope/swift)

[TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING](https://arxiv.org/pdf/2110.04366.pdf)

[人工智能大语言模型微调技术：SFT 监督微调、LoRA 微调方法、P-tuning v2 微调方法、Freeze 监督微调方法](https://cloud.tencent.com/developer/article/2338313)

[人工智能大语言模型微调技术：SFT 、LoRA 、Freeze 监督微调方法](https://www.toutiao.com/article/7256433230780531234/)

[大模型微调技术LoRA与QLoRA](https://blog.csdn.net/Gefangenes/article/details/131468405)

[QLoRA：4-bit级别的量化+LoRA方法，用3090在DB-GPT上打造基于33B LLM的个人知识库](https://zhuanlan.zhihu.com/p/634516004)

[LoRA和QLoRA微调语言大模型：数百次实验后的见解](https://zhuanlan.zhihu.com/p/664912829)

[LoRA 微调语言大模型的实用技巧](https://my.oschina.net/oneflow/blog/10320748)

[微调语言大模型选LoRA还是全参数？基于LLaMA 2深度分析](https://hub.baai.ac.cn/view/32405)

[通义千问Qwen-7B效果如何？Firefly微调实践，效果出色](https://mp.weixin.qq.com/s/5OAx83j6Op299XAfa496ww)

## Quantization

[<mark><font color="red" face="Times New Roman" size=5>GPTQ</font></mark>](https://arxiv.org/abs/2210.17323)

[<mark><font color="red" face="Times New Roman" size=5>AWQ</font></mark>](https://arxiv.org/abs/2306.00978)

[<mark><font color="red" face="Times New Roman" size=5>SqueezeLLM</font></mark>](https://github.com/SqueezeAILab/SqueezeLLM)

<mark><font color="red" face="Times New Roman" size=5>FP8 KV Cache</font></mark>

[用 bitsandbytes、4 比特量化和 QLoRA 打造亲民的 LLM](https://huggingface.co/blog/zh/4bit-transformers-bitsandbytes)

[大规模 Transformer 模型 8 比特矩阵乘简介 - 基于 Hugging Face Transformers、Accelerate 以及 bitsandbytes](https://huggingface.co/blog/zh/hf-bitsandbytes-integration)

## Training / Inference

[<mark><font color="red" face="Times New Roman" size=5>llama.cpp</font></mark>](https://github.com/ggerganov/llama.cpp)

[大模型部署工具 llama.cpp](https://www.chenshaowen.com/blog/llama-cpp-that-is-a-llm-deployment-tool.html)

[<mark><font color="red" face="Times New Roman" size=5>Transformers</font></mark>](https://github.com/huggingface/transformers)

[<mark><font color="red" face="Times New Roman" size=5>DeepSpeed</font></mark>](https://github.com/microsoft/DeepSpeed)

[DeepSpeed之ZeRO系列：将显存优化进行到底](https://zhuanlan.zhihu.com/p/513571706)

[DeepSpeed: Extreme-scale model training for everyone](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)

[Getting Started with DeepSpeed for Inferencing Transformer based Models](https://www.deepspeed.ai/tutorials/inference-tutorial/)

<mark><font color="red" face="Times New Roman" size=5>torchrun</font></mark>

[<mark><font color="red" face="Times New Roman" size=5>Accelerate</font></mark>](https://github.com/huggingface/accelerate)

[<mark><font color="red" face="Times New Roman" size=5>onnxruntime</font></mark>](https://github.com/microsoft/onnxruntime)

[如何将Pytorch模型转ONNX格式并使用OnnxRuntime推理](https://bbs.huaweicloud.com/blogs/180532)

[<mark><font color="red" face="Times New Roman" size=5>openvino</font></mark>](https://github.com/openvinotoolkit/openvino)

[<mark><font color="red" face="Times New Roman" size=5>mindspore</font></mark>](https://github.com/mindspore-ai/mindspore)

[<mark><font color="red" face="Times New Roman" size=5>PaddlePaddle</font></mark>](https://github.com/PaddlePaddle)

[大模型推理框架概述](https://blog.csdn.net/qq_27590277/article/details/133759166)

[A guide to LLM inference and performance](https://www.baseten.co/blog/llm-transformer-inference-guide/)

**PyTorch 显存释放：** 推理时禁用了梯度计算，推理后显式地释放未使用的显存资源，避免显存累积

```python
class EmbeddingTorchBackend(EmbeddingBackend):
    def __init__(self, use_cpu: bool):
        super().__init__(use_cpu)
        self.return_tensors = "pt"
        self._model = AutoModel.from_pretrained(LOCAL_EMBED_PATH, return_dict=False)
        if use_cpu:
            self.device = torch.device('cpu')
        else:
            self.device = torch.device('cuda')
        self._model = self._model.to(self.device)
        print("embedding device:", self.device)

    def get_embedding(self, sentences, max_length):
        inputs_pt = self._tokenizer(sentences, padding=True, truncation=True, max_length=max_length,
                                    return_tensors=self.return_tensors)
        inputs_pt = {k: v.to(self.device) for k, v in inputs_pt.items()}
        start_time = time.time()
        with torch.no_grad():  # 禁用梯度计算
            outputs_pt = self._model(**inputs_pt)
        torch.cuda.empty_cache()  # 释放未使用显存
        debug_logger.info(f"torch embedding infer time: {time.time() - start_time}")
        embedding = outputs_pt[0][:, 0].cpu().detach().numpy()
        debug_logger.info(f'embedding shape: {embedding.shape}')
        norm_arr = np.linalg.norm(embedding, axis=1, keepdims=True)
        embeddings_normalized = embedding / norm_arr
        return embeddings_normalized.tolist()
```

## LLM serving engines ([performance benchmark](https://buildkite.com/vllm/performance-benchmark/builds/3924)):

[<mark><font color="red" face="Times New Roman" size=5>vllm</font></mark>](https://github.com/vllm-project/vllm)

[<mark><font color="red" face="Times New Roman" size=5>TensorRT-LLM</font></mark>](https://github.com/NVIDIA/TensorRT-LLM)

[<mark><font color="red" face="Times New Roman" size=5>text-generation-inference</font></mark>](https://github.com/huggingface/text-generation-inference)

[<mark><font color="red" face="Times New Roman" size=5>lmdeploy</font></mark>](https://github.com/InternLM/lmdeploy)

## Toolchain

[<mark><font color="red" face="Times New Roman" size=5>LangChain: a framework for developing applications powered by LLMs</font></mark>](https://github.com/langchain-ai/langchain)

[LangChain for Java: Supercharge your Java application with the power of LLMs](https://github.com/langchain4j/langchain4j)

[LangChain 与 LangSmith：构建与微调支持LLM的智能应用双重攻略](https://blog.csdn.net/FrenzyTechAI/article/details/132695264)

[<mark><font color="red" face="Times New Roman" size=5>LlamaIndex: data framework for your LLM application</font></mark>](https://github.com/run-llama/llama_index)

[<mark><font color="red" face="Times New Roman" size=5>llmware: The Ultimate Toolkit for Building LLM Apps</font></mark>](https://github.com/llmware-ai/llmware)

[<mark><font color="red" face="Times New Roman" size=5>Ollama: Get up and running with large language models locally</font></mark>](https://github.com/ollama/ollama)

[<mark><font color="red" face="宋体" size=5>浦源OpenXLab</font></mark>](https://openxlab.org.cn/)

## Toolkit

[<mark><font color="red" face="Times New Roman" size=5>Weights & Biases</font></mark>](https://github.com/wandb/wandb)

[<mark><font color="red" face="Times New Roman" size=5>tensorboard</font></mark>](https://github.com/tensorflow/tensorboard)

[<mark><font color="red" face="Times New Roman" size=5>dvc</font></mark>](https://github.com/iterative/dvc)

[<mark><font color="red" face="Times New Roman" size=5>Kubeflow</font></mark>](https://github.com/kubeflow/kubeflow)

[<mark><font color="red" face="Times New Roman" size=5>mlflow</font></mark>](https://github.com/mlflow/mlflow)

## API Calls

[<mark><font color="red" face="Times New Roman" size=5>FastAPI</font></mark>](https://github.com/tiangolo/fastapi)

[<mark><font color="red" face="Times New Roman" size=5>Sanic</font></mark>](https://github.com/sanic-org/sanic)

#### [OpenAI API](https://github.com/openai/openai-python)

```python
from openai import OpenAI
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123",
)

models = client.models.list()
model = models.data[0].id

print(model)

completion = client.chat.completions.create(
  model= model,
  messages=[
    # {"role": "system", "content": ""},
    {"role": "user", "content": "hello"}
  ],
  temperature=0.7,
  max_tokens=4096,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=None
)

print(completion.choices[0].message.content)
```

#### [AzureOpenAI API](https://learn.microsoft.com/en-gb/azure/ai-services/openai/quickstart?pivots=programming-language-python&tabs=command-line%2Cpython-new#create-a-new-python-application)

通用：

```python
import os
from openai import AzureOpenAI

client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2024-02-01",
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)

chat_completion = client.chat.completions.create(
    model="DEPLOYMENT_NAME",
    messages=[{
                "role": "user",
                "content": "x=3, x+y等于多少",
            },
            {
                "role": "assistant",
                "content": "请告诉我y是多少"
            },
            {
                "role": "user",
                "content": "y是100，那结果是多少"
        }]
)

print(chat_completion.choices[0].message.content)
```
仅适用 `gpt-35-turbo-instruct`：
```python
import os
from openai import AzureOpenAI
    
client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),  
    api_version="2024-02-01",
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    )
    
deployment_name='REPLACE_WITH_YOUR_DEPLOYMENT_NAME' #This will correspond to the custom name you chose for your deployment when you deployed a model. Use a gpt-35-turbo-instruct deployment. 
    
# Send a completion call to generate an answer
print('Sending a test completion job')
start_phrase = 'Write a tagline for an ice cream shop. '
response = client.completions.create(model=deployment_name, prompt=start_phrase, max_tokens=10)
print(start_phrase+response.choices[0].text)
```

#### vllm API Calls Template

[vllm在线推理踩坑记](https://blog.csdn.net/smalllongonline/article/details/140174856)，[vLLM部署流式推理、openai接口调用、requests调用](https://blog.csdn.net/weixin_46398647/article/details/139963697)，[GLM-4-9B-Chat](https://github.com/vllm-project/vllm/issues/5306)


## WebUI

[<mark><font color="red" face="Times New Roman" size=5>Streamlit</font></mark>](https://github.com/streamlit/streamlit)

[<mark><font color="red" face="Times New Roman" size=5>gradio</font></mark>](https://github.com/gradio-app/gradio)

[gradio share=True 开放外部链接](https://blog.csdn.net/xxnnhcgdjy/article/details/132968705)

```
Could not create share link. Missing file: /data/jiangyy/miniconda3/envs/glm/lib/python3.10/site-packages/gradio/frpc_linux_amd64_v0.2. 

Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: 

1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64
2. Rename the downloaded file to: frpc_linux_amd64_v0.2
3. Move the file to this location: /data/jiangyy/miniconda3/envs/glm/lib/python3.10/site-packages/gradio
```

下载后无果：Could not create share link. Please check your internet connection or our status page: https://status.gradio.app

可继续尝试：https://github.com/gradio-app/gradio/pull/6091

最终解决：`chmod +x frpc_linux_amd64_v0.2`

```
Running on local URL:  http://127.0.0.1:8001
Running on public URL: https://c3f57738b574623792.gradio.live

This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)
```

## Others

#### Hyperparams

[分析transformer模型的参数量、计算量、中间激活、KV cache](https://zhuanlan.zhihu.com/p/624740065)

**batch size**

[Batch size对训练效果的影响](https://blog.csdn.net/weixin_42788078/article/details/120901079)

[神经网络中Batch和Epoch之间的区别](https://blog.csdn.net/weixin_38314865/article/details/84243343)

#### RLHF

[Implementing RLHF: Learning to Summarize with trlX](https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2#source-code)

[RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/abs/2309.00267)

[RLHF-PPO算法代码解析](https://zhuanlan.zhihu.com/p/650505023)

[offline rl, online rl, on-policy rl, off-policy rl](https://blog.csdn.net/qq_42743778/article/details/120063861)

![rl](https://raw.githubusercontent.com/yuy4o/yuy4o/main/figures/240613rl.png)

#### AGI

[WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)

#### Insights

**Bert 对比 LLM：**

Bert 优点：1双向编码的优势（考虑前后文的信息），2较小的数据和资源，缺点：1只能用于理解任务

LLM 优点：1能理解能生成，2多样化任务（翻译、摘要、对话）不用针对每个任务单独微调，缺点：数据和资源消耗大