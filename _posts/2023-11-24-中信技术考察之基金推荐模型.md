---
title: 中信技术考察之基金推荐模型
author: yuyao
date: 2023-11-24 20:49:20 +0800 
categories: [Projects, zhongxin]
tags: [ml]
---

```python
import csv
import copy
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
```

## 1. 处理客户数据


```python
with open('./data/客户数据.csv', 'r', encoding='utf-8') as file:
    # 创建CSV读取器对象
    csv_reader = csv.reader(file)
    # 跳过标题行
    customer_headers = next(csv_reader)[1:]
    # print(f'Headers: {customer_headers}')
    customer_list = []
    # 遍历每一行并打印
    for row in csv_reader:
        customer_list.append(row[1:])
customer_df = pd.DataFrame(customer_list, columns=customer_headers)
customer_df  
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>客户编号</th>
      <th>客户公司名称</th>
      <th>客户公司编号</th>
      <th>feature1</th>
      <th>feature2</th>
      <th>feature3</th>
      <th>feature4</th>
      <th>feature5</th>
      <th>feature6</th>
      <th>feature7</th>
      <th>...</th>
      <th>feature35</th>
      <th>feature36</th>
      <th>feature37</th>
      <th>feature38</th>
      <th>feature39</th>
      <th>feature40</th>
      <th>第一选择基金</th>
      <th>第二选择基金</th>
      <th>第三选择基金</th>
      <th>第四选择基金</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>C0001</td>
      <td>公司20</td>
      <td>4040100</td>
      <td>9</td>
      <td>9</td>
      <td>15.44036767</td>
      <td>193</td>
      <td>0.166081948</td>
      <td>1</td>
      <td>11</td>
      <td>...</td>
      <td>1</td>
      <td>3</td>
      <td>10</td>
      <td>43</td>
      <td>56</td>
      <td>44.99447821</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>C0002</td>
      <td>公司3</td>
      <td>90601</td>
      <td>10</td>
      <td>69</td>
      <td>24.19789929</td>
      <td>86</td>
      <td>0.327464351</td>
      <td>1</td>
      <td>17</td>
      <td>...</td>
      <td>8</td>
      <td>3</td>
      <td>11</td>
      <td>22</td>
      <td>74</td>
      <td>54.38582584</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>C0003</td>
      <td>公司18</td>
      <td>3272481</td>
      <td>3</td>
      <td>61</td>
      <td>6.773532386</td>
      <td>201</td>
      <td>0.482634837</td>
      <td>1</td>
      <td>2</td>
      <td>...</td>
      <td>5</td>
      <td>6</td>
      <td>10</td>
      <td>44</td>
      <td>19</td>
      <td>9.35864196</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>C0004</td>
      <td>公司9</td>
      <td>817216</td>
      <td>3</td>
      <td>9</td>
      <td>6.375025793</td>
      <td>92</td>
      <td>0.545685448</td>
      <td>1</td>
      <td>3</td>
      <td>...</td>
      <td>10</td>
      <td>5</td>
      <td>3</td>
      <td>71</td>
      <td>82</td>
      <td>55.20456675</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>C0005</td>
      <td>公司6</td>
      <td>363609</td>
      <td>3</td>
      <td>65</td>
      <td>4.183100481</td>
      <td>195</td>
      <td>0.657033027</td>
      <td>1</td>
      <td>16</td>
      <td>...</td>
      <td>3</td>
      <td>7</td>
      <td>11</td>
      <td>3</td>
      <td>31</td>
      <td>41.24379255</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>11995</th>
      <td>C11996</td>
      <td>公司16</td>
      <td>2585664</td>
      <td>7</td>
      <td>37</td>
      <td>7.110256464</td>
      <td>173</td>
      <td>0.320702523</td>
      <td>1</td>
      <td>4</td>
      <td>...</td>
      <td>6</td>
      <td>6</td>
      <td>12</td>
      <td>75</td>
      <td>6</td>
      <td>86.15755992</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>11996</th>
      <td>C11997</td>
      <td>公司4</td>
      <td>161604</td>
      <td>5</td>
      <td>60</td>
      <td>10.05886805</td>
      <td>74</td>
      <td>0.986376089</td>
      <td>1</td>
      <td>6</td>
      <td>...</td>
      <td>12</td>
      <td>4</td>
      <td>12</td>
      <td>82</td>
      <td>16</td>
      <td>11.28849184</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>11997</th>
      <td>C11998</td>
      <td>公司19</td>
      <td>3644281</td>
      <td>4</td>
      <td>37</td>
      <td>25.60124899</td>
      <td>66</td>
      <td>0.990199579</td>
      <td>1</td>
      <td>14</td>
      <td>...</td>
      <td>8</td>
      <td>5</td>
      <td>4</td>
      <td>34</td>
      <td>85</td>
      <td>48.17353183</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>11998</th>
      <td>C11999</td>
      <td>公司7</td>
      <td>494209</td>
      <td>10</td>
      <td>60</td>
      <td>30.56264901</td>
      <td>34</td>
      <td>0.775651603</td>
      <td>1</td>
      <td>6</td>
      <td>...</td>
      <td>12</td>
      <td>7</td>
      <td>10</td>
      <td>18</td>
      <td></td>
      <td>6.35029667</td>
      <td>J0076</td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>11999</th>
      <td>C12000</td>
      <td>公司14</td>
      <td>1979649</td>
      <td>1</td>
      <td>30</td>
      <td>12.6276525</td>
      <td>86</td>
      <td>0.233256336</td>
      <td>1</td>
      <td>12</td>
      <td>...</td>
      <td>4</td>
      <td>5</td>
      <td>9</td>
      <td>46</td>
      <td>45</td>
      <td>18.21188006</td>
      <td>J0139</td>
      <td>J0034</td>
      <td></td>
      <td></td>
    </tr>
  </tbody>
</table>
<p>12000 rows × 47 columns</p>
</div>




```python
# 处理一：筛选出训练数据
customer_list = customer_df.values.tolist()
print(len(customer_list))
customer_list = [row for row in customer_list if row[-4][0] == 'J']
print(len(customer_list))
```

    12000
    4780



```python
# 处理二：删除客户及公司信息 只保留特征
customer_list = [row[3:] for row in customer_list]
```


```python
# 处理三：复制
# 因为有4780行第一选择基金的数据，每行对应200支基金，训练数据有1000000数据量。故筛选至少选择2支基金的客户作为训练数据以减少训练成本 1704行每行对应200支基金
new_customer_list = []
strlist = ['J{:04d}'.format(i) for i in range(1, 201)]
for i in range(len(customer_list)):
    # 17800 = 89 * 200
    if customer_list[i][-1][0] == 'J':
    # ['J0116', 'J0029', 'J0133', 'J0156']
        #print(customer_list[i][-4:])
        for j in range(1, 5):
            tmp = copy.deepcopy(customer_list[i])
            tmp[-4] = customer_list[i][-5+j]
            tmp.append(j)
            new_customer_list.append(tmp)
        #print('-----------')
        for str in strlist:
            if str!= customer_list[i][-4] and str!= customer_list[i][-3] and str!= customer_list[i][-2] and str!= customer_list[i][-1]:
                tmp = copy.deepcopy(customer_list[i])
                tmp[-4] = str
                tmp.append(0)
                new_customer_list.append(tmp)
# 删除错例---客户数据.csv第10753行[J0163	J0012	J0139	J0012] 第二选择和第四选择重复
del new_customer_list[16403]
for i in range(len(customer_list)):
    # 256000 = 1280 * 200
    if customer_list[i][-3][0] == 'J' and customer_list[i][-2][0] != 'J' and customer_list[i][-1][0] != 'J':
        # ['J0132', 'J0081', '  ', '  ']
        #print(customer_list[i][-4:])
        for j in range(1, 3):
            tmp = copy.deepcopy(customer_list[i])
            tmp[-4] = customer_list[i][-5+j]
            tmp.append(j)
            new_customer_list.append(tmp)
        #print('-----------')
        for str in strlist:
            if str!= customer_list[i][-4] and str!= customer_list[i][-3]:
                tmp = copy.deepcopy(customer_list[i])
                tmp[-4] = str
                tmp.append(0)
                new_customer_list.append(tmp)          
    # 67000 = 335 * 200
    if customer_list[i][-2][0] == 'J' and customer_list[i][-1][0] != 'J':
        # ['J0188', 'J0191', 'J0068', '  ']
        #print(customer_list[i][-4:])
        for j in range(1, 4):
            tmp = copy.deepcopy(customer_list[i])
            tmp[-4] = customer_list[i][-5+j]
            tmp.append(j)
            new_customer_list.append(tmp)
        #print('-----------')
        for str in strlist:
            if str!= customer_list[i][-4] and str!= customer_list[i][-3] and str!= customer_list[i][-2]:
                tmp = copy.deepcopy(customer_list[i])
                tmp[-4] = str
                tmp.append(0)
                new_customer_list.append(tmp)
print(f'一共生成{len(new_customer_list)}条数据')
```

    一共生成340800条数据



```python
# 修改header
new_customer_headers = customer_headers[3:]+['选择排序']
for i in range(0, 40):
    new_customer_headers[i] = 'customer_' + new_customer_headers[i]
new_customer_headers[40] = '选择基金'   
new_customer_df = pd.DataFrame(new_customer_list, columns = new_customer_headers)
new_customer_df.drop(new_customer_df.columns[-4:-1], axis=1, inplace=True)
new_customer_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>customer_feature1</th>
      <th>customer_feature2</th>
      <th>customer_feature3</th>
      <th>customer_feature4</th>
      <th>customer_feature5</th>
      <th>customer_feature6</th>
      <th>customer_feature7</th>
      <th>customer_feature8</th>
      <th>customer_feature9</th>
      <th>customer_feature10</th>
      <th>...</th>
      <th>customer_feature33</th>
      <th>customer_feature34</th>
      <th>customer_feature35</th>
      <th>customer_feature36</th>
      <th>customer_feature37</th>
      <th>customer_feature38</th>
      <th>customer_feature39</th>
      <th>customer_feature40</th>
      <th>选择基金</th>
      <th>选择排序</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>37</td>
      <td>23.88129863</td>
      <td>71</td>
      <td>0.698777259</td>
      <td>1</td>
      <td>13</td>
      <td>194</td>
      <td>6.217688451</td>
      <td>55</td>
      <td>...</td>
      <td>4.810028598</td>
      <td>1</td>
      <td>7</td>
      <td>2</td>
      <td>10</td>
      <td>80</td>
      <td>40</td>
      <td>46.45630119</td>
      <td>J0045</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>37</td>
      <td>23.88129863</td>
      <td>71</td>
      <td>0.698777259</td>
      <td>1</td>
      <td>13</td>
      <td>194</td>
      <td>6.217688451</td>
      <td>55</td>
      <td>...</td>
      <td>4.810028598</td>
      <td>1</td>
      <td>7</td>
      <td>2</td>
      <td>10</td>
      <td>80</td>
      <td>40</td>
      <td>46.45630119</td>
      <td>J0109</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>37</td>
      <td>23.88129863</td>
      <td>71</td>
      <td>0.698777259</td>
      <td>1</td>
      <td>13</td>
      <td>194</td>
      <td>6.217688451</td>
      <td>55</td>
      <td>...</td>
      <td>4.810028598</td>
      <td>1</td>
      <td>7</td>
      <td>2</td>
      <td>10</td>
      <td>80</td>
      <td>40</td>
      <td>46.45630119</td>
      <td>J0063</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>37</td>
      <td>23.88129863</td>
      <td>71</td>
      <td>0.698777259</td>
      <td>1</td>
      <td>13</td>
      <td>194</td>
      <td>6.217688451</td>
      <td>55</td>
      <td>...</td>
      <td>4.810028598</td>
      <td>1</td>
      <td>7</td>
      <td>2</td>
      <td>10</td>
      <td>80</td>
      <td>40</td>
      <td>46.45630119</td>
      <td>J0099</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>37</td>
      <td>23.88129863</td>
      <td>71</td>
      <td>0.698777259</td>
      <td>1</td>
      <td>13</td>
      <td>194</td>
      <td>6.217688451</td>
      <td>55</td>
      <td>...</td>
      <td>4.810028598</td>
      <td>1</td>
      <td>7</td>
      <td>2</td>
      <td>10</td>
      <td>80</td>
      <td>40</td>
      <td>46.45630119</td>
      <td>J0001</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>340795</th>
      <td>1</td>
      <td>30</td>
      <td>12.6276525</td>
      <td>86</td>
      <td>0.233256336</td>
      <td>1</td>
      <td>12</td>
      <td>85</td>
      <td>3.740509451</td>
      <td>55</td>
      <td>...</td>
      <td>9.660924457</td>
      <td>1</td>
      <td>4</td>
      <td>5</td>
      <td>9</td>
      <td>46</td>
      <td>45</td>
      <td>18.21188006</td>
      <td>J0196</td>
      <td>0</td>
    </tr>
    <tr>
      <th>340796</th>
      <td>1</td>
      <td>30</td>
      <td>12.6276525</td>
      <td>86</td>
      <td>0.233256336</td>
      <td>1</td>
      <td>12</td>
      <td>85</td>
      <td>3.740509451</td>
      <td>55</td>
      <td>...</td>
      <td>9.660924457</td>
      <td>1</td>
      <td>4</td>
      <td>5</td>
      <td>9</td>
      <td>46</td>
      <td>45</td>
      <td>18.21188006</td>
      <td>J0197</td>
      <td>0</td>
    </tr>
    <tr>
      <th>340797</th>
      <td>1</td>
      <td>30</td>
      <td>12.6276525</td>
      <td>86</td>
      <td>0.233256336</td>
      <td>1</td>
      <td>12</td>
      <td>85</td>
      <td>3.740509451</td>
      <td>55</td>
      <td>...</td>
      <td>9.660924457</td>
      <td>1</td>
      <td>4</td>
      <td>5</td>
      <td>9</td>
      <td>46</td>
      <td>45</td>
      <td>18.21188006</td>
      <td>J0198</td>
      <td>0</td>
    </tr>
    <tr>
      <th>340798</th>
      <td>1</td>
      <td>30</td>
      <td>12.6276525</td>
      <td>86</td>
      <td>0.233256336</td>
      <td>1</td>
      <td>12</td>
      <td>85</td>
      <td>3.740509451</td>
      <td>55</td>
      <td>...</td>
      <td>9.660924457</td>
      <td>1</td>
      <td>4</td>
      <td>5</td>
      <td>9</td>
      <td>46</td>
      <td>45</td>
      <td>18.21188006</td>
      <td>J0199</td>
      <td>0</td>
    </tr>
    <tr>
      <th>340799</th>
      <td>1</td>
      <td>30</td>
      <td>12.6276525</td>
      <td>86</td>
      <td>0.233256336</td>
      <td>1</td>
      <td>12</td>
      <td>85</td>
      <td>3.740509451</td>
      <td>55</td>
      <td>...</td>
      <td>9.660924457</td>
      <td>1</td>
      <td>4</td>
      <td>5</td>
      <td>9</td>
      <td>46</td>
      <td>45</td>
      <td>18.21188006</td>
      <td>J0200</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>340800 rows × 42 columns</p>
</div>




```python
# 将 DataFrame 中所有缺失值和极值先全部转为np.nan
new_customer_list = new_customer_df.values.tolist()
for row in new_customer_list:
    # 提取dataframe中数值数据 从第三列起
    for i in range(0, len(row) - 2):
        if row[i] != '':
            row[i] = float(row[i])
            # 极值999999999转为np.nan
            if row[i] > 1000 or row[i] < -1000:
                row[i] = np.nan
for row in new_customer_list:
    for i in range(0, len(row) - 2):
        if row[i] == '':
            # 缺失值转为np.nan
            row[i] = np.nan
```


```python
# 计算所有列的平均值
new_customer_df = pd.DataFrame(new_customer_list, columns = new_customer_headers[:-4] +new_customer_headers[-1:])
column_means = new_customer_df.iloc[:,:-2].mean(skipna=True)
print(column_means)
```

    customer_feature1       6.360880
    customer_feature2      36.540284
    customer_feature3      17.201055
    customer_feature4     105.000593
    customer_feature5       0.498709
    customer_feature6       1.000000
    customer_feature7       9.405694
    customer_feature8     100.980251
    customer_feature9      16.913861
    customer_feature10     34.972123
    customer_feature11      0.498026
    customer_feature12      6.518365
    customer_feature13     35.883363
    customer_feature14     10.118418
    customer_feature15     16.726703
    customer_feature16     47.693500
    customer_feature17     16.657743
    customer_feature18     48.414837
    customer_feature19     46.688786
    customer_feature20      9.528190
    customer_feature21     10.009301
    customer_feature22     14.929339
    customer_feature23     49.011289
    customer_feature24     35.924674
    customer_feature25      9.861468
    customer_feature26      0.000000
    customer_feature27     45.171292
    customer_feature28      9.976480
    customer_feature29     45.946405
    customer_feature30      2.985731
    customer_feature31      1.000000
    customer_feature32     49.080166
    customer_feature33     10.037611
    customer_feature34      1.000000
    customer_feature35      6.550535
    customer_feature36      4.053039
    customer_feature37      6.507101
    customer_feature38     49.062278
    customer_feature39     48.582398
    customer_feature40     46.878168
    dtype: float64



```python
for row in new_customer_list:
    for i in range(0, len(row) - 2):
        if np.isnan(row[i]):
            # 缺失值转为np.nan
            row[i] = column_means[i]
```


```python
# list 转回 dataframe
new_customer_df = pd.DataFrame(new_customer_list, columns = new_customer_headers[:-4] +new_customer_headers[-1:])
```


```python
# 处理五：将特征重新缩放到零均值和单位方差来标准化数据
new_customer_df_scaled = new_customer_df.iloc[:,:-2]
new_customer_df_scaled = pd.DataFrame(StandardScaler().fit_transform(new_customer_df_scaled), columns=new_customer_df_scaled.columns)
# 将标准化后的40个特征和最后选择拼接
new_customer_df = pd.concat([new_customer_df_scaled, new_customer_df.iloc[:, -2:]], axis=1)
new_customer_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>customer_feature1</th>
      <th>customer_feature2</th>
      <th>customer_feature3</th>
      <th>customer_feature4</th>
      <th>customer_feature5</th>
      <th>customer_feature6</th>
      <th>customer_feature7</th>
      <th>customer_feature8</th>
      <th>customer_feature9</th>
      <th>customer_feature10</th>
      <th>...</th>
      <th>customer_feature33</th>
      <th>customer_feature34</th>
      <th>customer_feature35</th>
      <th>customer_feature36</th>
      <th>customer_feature37</th>
      <th>customer_feature38</th>
      <th>customer_feature39</th>
      <th>customer_feature40</th>
      <th>选择基金</th>
      <th>选择排序</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.255141</td>
      <td>0.022313</td>
      <td>0.686722</td>
      <td>-0.580099</td>
      <td>0.696518</td>
      <td>0.0</td>
      <td>0.701266</td>
      <td>1.584568</td>
      <td>-1.094460</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.937251</td>
      <td>0.0</td>
      <td>0.130054</td>
      <td>-1.037672</td>
      <td>1.008106</td>
      <td>1.113990</td>
      <td>-0.309102</td>
      <td>-0.016149</td>
      <td>J0045</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.255141</td>
      <td>0.022313</td>
      <td>0.686722</td>
      <td>-0.580099</td>
      <td>0.696518</td>
      <td>0.0</td>
      <td>0.701266</td>
      <td>1.584568</td>
      <td>-1.094460</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.937251</td>
      <td>0.0</td>
      <td>0.130054</td>
      <td>-1.037672</td>
      <td>1.008106</td>
      <td>1.113990</td>
      <td>-0.309102</td>
      <td>-0.016149</td>
      <td>J0109</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.255141</td>
      <td>0.022313</td>
      <td>0.686722</td>
      <td>-0.580099</td>
      <td>0.696518</td>
      <td>0.0</td>
      <td>0.701266</td>
      <td>1.584568</td>
      <td>-1.094460</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.937251</td>
      <td>0.0</td>
      <td>0.130054</td>
      <td>-1.037672</td>
      <td>1.008106</td>
      <td>1.113990</td>
      <td>-0.309102</td>
      <td>-0.016149</td>
      <td>J0063</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.255141</td>
      <td>0.022313</td>
      <td>0.686722</td>
      <td>-0.580099</td>
      <td>0.696518</td>
      <td>0.0</td>
      <td>0.701266</td>
      <td>1.584568</td>
      <td>-1.094460</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.937251</td>
      <td>0.0</td>
      <td>0.130054</td>
      <td>-1.037672</td>
      <td>1.008106</td>
      <td>1.113990</td>
      <td>-0.309102</td>
      <td>-0.016149</td>
      <td>J0099</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.255141</td>
      <td>0.022313</td>
      <td>0.686722</td>
      <td>-0.580099</td>
      <td>0.696518</td>
      <td>0.0</td>
      <td>0.701266</td>
      <td>1.584568</td>
      <td>-1.094460</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.937251</td>
      <td>0.0</td>
      <td>0.130054</td>
      <td>-1.037672</td>
      <td>1.008106</td>
      <td>1.113990</td>
      <td>-0.309102</td>
      <td>-0.016149</td>
      <td>J0001</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>340795</th>
      <td>-1.542959</td>
      <td>-0.317437</td>
      <td>-0.470141</td>
      <td>-0.324177</td>
      <td>-0.924150</td>
      <td>0.0</td>
      <td>0.506162</td>
      <td>-0.272220</td>
      <td>-1.347931</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.067536</td>
      <td>0.0</td>
      <td>-0.738007</td>
      <td>0.478624</td>
      <td>0.719490</td>
      <td>-0.110265</td>
      <td>-0.129023</td>
      <td>-1.097325</td>
      <td>J0196</td>
      <td>0</td>
    </tr>
    <tr>
      <th>340796</th>
      <td>-1.542959</td>
      <td>-0.317437</td>
      <td>-0.470141</td>
      <td>-0.324177</td>
      <td>-0.924150</td>
      <td>0.0</td>
      <td>0.506162</td>
      <td>-0.272220</td>
      <td>-1.347931</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.067536</td>
      <td>0.0</td>
      <td>-0.738007</td>
      <td>0.478624</td>
      <td>0.719490</td>
      <td>-0.110265</td>
      <td>-0.129023</td>
      <td>-1.097325</td>
      <td>J0197</td>
      <td>0</td>
    </tr>
    <tr>
      <th>340797</th>
      <td>-1.542959</td>
      <td>-0.317437</td>
      <td>-0.470141</td>
      <td>-0.324177</td>
      <td>-0.924150</td>
      <td>0.0</td>
      <td>0.506162</td>
      <td>-0.272220</td>
      <td>-1.347931</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.067536</td>
      <td>0.0</td>
      <td>-0.738007</td>
      <td>0.478624</td>
      <td>0.719490</td>
      <td>-0.110265</td>
      <td>-0.129023</td>
      <td>-1.097325</td>
      <td>J0198</td>
      <td>0</td>
    </tr>
    <tr>
      <th>340798</th>
      <td>-1.542959</td>
      <td>-0.317437</td>
      <td>-0.470141</td>
      <td>-0.324177</td>
      <td>-0.924150</td>
      <td>0.0</td>
      <td>0.506162</td>
      <td>-0.272220</td>
      <td>-1.347931</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.067536</td>
      <td>0.0</td>
      <td>-0.738007</td>
      <td>0.478624</td>
      <td>0.719490</td>
      <td>-0.110265</td>
      <td>-0.129023</td>
      <td>-1.097325</td>
      <td>J0199</td>
      <td>0</td>
    </tr>
    <tr>
      <th>340799</th>
      <td>-1.542959</td>
      <td>-0.317437</td>
      <td>-0.470141</td>
      <td>-0.324177</td>
      <td>-0.924150</td>
      <td>0.0</td>
      <td>0.506162</td>
      <td>-0.272220</td>
      <td>-1.347931</td>
      <td>0.973191</td>
      <td>...</td>
      <td>-0.067536</td>
      <td>0.0</td>
      <td>-0.738007</td>
      <td>0.478624</td>
      <td>0.719490</td>
      <td>-0.110265</td>
      <td>-0.129023</td>
      <td>-1.097325</td>
      <td>J0200</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>340800 rows × 42 columns</p>
</div>



## 2. 处理基金数据


```python
with open('./data/基金数据.csv', 'r', encoding='utf-8') as file:
    # 创建CSV读取器对象
    csv_reader = csv.reader(file)
    # 跳过标题行
    fund_headers = next(csv_reader)[1:]
    # print(f'Headers: {fund_headers}')
    fund_list = []
    # 遍历每一行并打印
    for row in csv_reader:
        fund_list.append(row[1:])
fund_df = pd.DataFrame(fund_list, columns=fund_headers)
fund_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>基金代码</th>
      <th>基金公司</th>
      <th>基金成立时间</th>
      <th>净值</th>
      <th>feature1</th>
      <th>feature2</th>
      <th>feature3</th>
      <th>feature4</th>
      <th>feature5</th>
      <th>feature6</th>
      <th>...</th>
      <th>feature21</th>
      <th>feature22</th>
      <th>feature23</th>
      <th>feature24</th>
      <th>feature25</th>
      <th>feature26</th>
      <th>feature27</th>
      <th>feature28</th>
      <th>feature29</th>
      <th>feature30</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>J0001</td>
      <td>基金公司32</td>
      <td>2021/10/28</td>
      <td>-3.483643378</td>
      <td>1</td>
      <td>6</td>
      <td>21</td>
      <td>7</td>
      <td>3</td>
      <td>14.0063104</td>
      <td>...</td>
      <td>1.096101546</td>
      <td>1.245983443</td>
      <td>2</td>
      <td>-2.562946447</td>
      <td>4</td>
      <td>6</td>
      <td>12</td>
      <td>12</td>
      <td>2.24522696</td>
      <td>-1.74427907</td>
    </tr>
    <tr>
      <th>1</th>
      <td>J0002</td>
      <td>基金公司7</td>
      <td>2023/4/7</td>
      <td>-2.01121916</td>
      <td>2</td>
      <td>9</td>
      <td>19</td>
      <td>4</td>
      <td>10</td>
      <td>-3.581032341</td>
      <td>...</td>
      <td>-3.471400767</td>
      <td>1.557628419</td>
      <td>5</td>
      <td>-0.183853669</td>
      <td>6</td>
      <td>3</td>
      <td>11</td>
      <td>9</td>
      <td>2.491505767</td>
      <td>2.959278664</td>
    </tr>
    <tr>
      <th>2</th>
      <td>J0003</td>
      <td>基金公司32</td>
      <td>2021/6/24</td>
      <td>1.54954112</td>
      <td>3</td>
      <td>7</td>
      <td>2</td>
      <td>8</td>
      <td>7</td>
      <td>9.535030668</td>
      <td>...</td>
      <td>-0.223731795</td>
      <td>2.230497658</td>
      <td>19</td>
      <td>8.76975046</td>
      <td>7</td>
      <td>12</td>
      <td>21</td>
      <td>6</td>
      <td>2.787080936</td>
      <td>5.513654932</td>
    </tr>
    <tr>
      <th>3</th>
      <td>J0004</td>
      <td>基金公司3</td>
      <td>2022/11/22</td>
      <td>-3.000416018</td>
      <td>3</td>
      <td>20</td>
      <td>3</td>
      <td>9</td>
      <td>4</td>
      <td>-6.844948334</td>
      <td>...</td>
      <td>-5.377407589</td>
      <td>0.111784599</td>
      <td>12</td>
      <td>4.246656241</td>
      <td>1</td>
      <td>11</td>
      <td>19</td>
      <td>3</td>
      <td>1.531418859</td>
      <td>5.766965363</td>
    </tr>
    <tr>
      <th>4</th>
      <td>J0005</td>
      <td>基金公司4</td>
      <td>2023/9/22</td>
      <td>-2.388125665</td>
      <td>1</td>
      <td>16</td>
      <td>8</td>
      <td>8</td>
      <td>1</td>
      <td>9.637912079</td>
      <td>...</td>
      <td>-1.487962684</td>
      <td>2.326857149</td>
      <td>6</td>
      <td>7.025243136</td>
      <td>2</td>
      <td>9</td>
      <td>5</td>
      <td>2</td>
      <td>2.884185111</td>
      <td>2.918918908</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 34 columns</p>
</div>




```python
# 希望将‘基金成立时间’作为一个特征用于训练，对‘基金成立时间’进行独立编码
date = fund_df.iloc[:, 2:3]
# 将日期字符串转换为日期时间对象
date['date'] = pd.to_datetime(date['基金成立时间'])
# 提取年、月、日等信息作为新的特征
date['year'] = date['date'].dt.year
date['month'] = date['date'].dt.month
date['day'] = date['date'].dt.day
# 独热编码年份、月份等特征
date_encoded = pd.get_dummies(date, columns=['year', 'month', 'day']) # 200 rows × 48 columns

# 希望将‘基金公司’作为一个特征用于训练，对‘基金公司’进行独立编码
company = fund_df.iloc[:, 1:2]
company_encoded = pd.get_dummies(company) # 200 rows × 58 columns

# 汇总独立编码结果
fund_df.drop(['基金公司', '基金成立时间'], axis=1, inplace=True)
fund_df = pd.concat([fund_df, company_encoded, date_encoded], axis=1)
fund_df['净值'] = fund_df.pop('净值')
fund_df
```





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>基金代码</th>
      <th>feature1</th>
      <th>feature2</th>
      <th>feature3</th>
      <th>feature4</th>
      <th>feature5</th>
      <th>feature6</th>
      <th>feature7</th>
      <th>feature8</th>
      <th>feature9</th>
      <th>...</th>
      <th>day_23</th>
      <th>day_24</th>
      <th>day_25</th>
      <th>day_26</th>
      <th>day_27</th>
      <th>day_28</th>
      <th>day_29</th>
      <th>day_30</th>
      <th>day_31</th>
      <th>净值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>J0001</td>
      <td>1</td>
      <td>6</td>
      <td>21</td>
      <td>7</td>
      <td>3</td>
      <td>14.0063104</td>
      <td>3</td>
      <td>3</td>
      <td>8</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-3.483643378</td>
    </tr>
    <tr>
      <th>1</th>
      <td>J0002</td>
      <td>2</td>
      <td>9</td>
      <td>19</td>
      <td>4</td>
      <td>10</td>
      <td>-3.581032341</td>
      <td>1</td>
      <td>5</td>
      <td>10</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-2.01121916</td>
    </tr>
    <tr>
      <th>2</th>
      <td>J0003</td>
      <td>3</td>
      <td>7</td>
      <td>2</td>
      <td>8</td>
      <td>7</td>
      <td>9.535030668</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1.54954112</td>
    </tr>
    <tr>
      <th>3</th>
      <td>J0004</td>
      <td>3</td>
      <td>20</td>
      <td>3</td>
      <td>9</td>
      <td>4</td>
      <td>-6.844948334</td>
      <td>2</td>
      <td>10</td>
      <td>10</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-3.000416018</td>
    </tr>
    <tr>
      <th>4</th>
      <td>J0005</td>
      <td>1</td>
      <td>16</td>
      <td>8</td>
      <td>8</td>
      <td>1</td>
      <td>9.637912079</td>
      <td>1</td>
      <td>5</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-2.388125665</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>195</th>
      <td>J0196</td>
      <td>3</td>
      <td>17</td>
      <td>13</td>
      <td>5</td>
      <td>2</td>
      <td>-1.537449111</td>
      <td>1</td>
      <td>11</td>
      <td>9</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>8.075655166</td>
    </tr>
    <tr>
      <th>196</th>
      <td>J0197</td>
      <td>3</td>
      <td>16</td>
      <td>13</td>
      <td>9</td>
      <td>11</td>
      <td>-3.686967574</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>7.724697415</td>
    </tr>
    <tr>
      <th>197</th>
      <td>J0198</td>
      <td>2</td>
      <td>19</td>
      <td>17</td>
      <td>1</td>
      <td>3</td>
      <td>11.25684561</td>
      <td>1</td>
      <td>1</td>
      <td>8</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-1.767100969</td>
    </tr>
    <tr>
      <th>198</th>
      <td>J0199</td>
      <td>2</td>
      <td>18</td>
      <td>21</td>
      <td>3</td>
      <td>13</td>
      <td>-1.866035653</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-2.661014584</td>
    </tr>
    <tr>
      <th>199</th>
      <td>J0200</td>
      <td>2</td>
      <td>18</td>
      <td>14</td>
      <td>1</td>
      <td>2</td>
      <td>4.948720634</td>
      <td>1</td>
      <td>7</td>
      <td>8</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4.231475045</td>
    </tr>
  </tbody>
</table>
<p>200 rows × 138 columns</p>
</div>




```python
# 处理五：根据选择基金列 构建基金信息表
fund_list = fund_df.values.tolist()
fund_choice = new_customer_df.iloc[:, -2].tolist()
concat_fund_list = []
for j in range(len(fund_choice)):
    for i in range(len(fund_list)):
        if fund_list[i][0] == fund_choice[j]:
            concat_fund_list.append(fund_list[i])
# fund_headers = fund_headers[:1] + fund_headers[4:] + fund_headers[3:4]
concat_fund_df = pd.DataFrame(concat_fund_list, columns=fund_df.columns)
```


```python
# 对基金信息表进行特征标准化
concat_fund_df_scaled = pd.DataFrame(StandardScaler().fit_transform(concat_fund_df.iloc[:, 1:31]), columns=concat_fund_df.iloc[:, 1:31].columns)
concat_fund_df_scaled
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature1</th>
      <th>feature2</th>
      <th>feature3</th>
      <th>feature4</th>
      <th>feature5</th>
      <th>feature6</th>
      <th>feature7</th>
      <th>feature8</th>
      <th>feature9</th>
      <th>feature10</th>
      <th>...</th>
      <th>feature21</th>
      <th>feature22</th>
      <th>feature23</th>
      <th>feature24</th>
      <th>feature25</th>
      <th>feature26</th>
      <th>feature27</th>
      <th>feature28</th>
      <th>feature29</th>
      <th>feature30</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.200640</td>
      <td>0.712252</td>
      <td>-1.076222</td>
      <td>-1.248437</td>
      <td>-0.334457</td>
      <td>0.956536</td>
      <td>-1.278443</td>
      <td>-0.047607</td>
      <td>0.958710</td>
      <td>-1.256648</td>
      <td>...</td>
      <td>0.025146</td>
      <td>0.877876</td>
      <td>0.460757</td>
      <td>-0.839008</td>
      <td>1.528116</td>
      <td>-1.419769</td>
      <td>1.612192</td>
      <td>0.510028</td>
      <td>0.438328</td>
      <td>-0.922319</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.200640</td>
      <td>-0.500829</td>
      <td>-1.231746</td>
      <td>-0.938266</td>
      <td>-0.981792</td>
      <td>-0.654327</td>
      <td>-1.278443</td>
      <td>0.269773</td>
      <td>0.958710</td>
      <td>1.051785</td>
      <td>...</td>
      <td>0.098985</td>
      <td>-1.406773</td>
      <td>0.797690</td>
      <td>1.288207</td>
      <td>-1.517962</td>
      <td>-0.292968</td>
      <td>0.374425</td>
      <td>-0.568255</td>
      <td>-1.662697</td>
      <td>0.292734</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.200640</td>
      <td>-0.327532</td>
      <td>-0.765175</td>
      <td>-0.317925</td>
      <td>-0.981792</td>
      <td>0.304544</td>
      <td>1.203971</td>
      <td>0.587154</td>
      <td>1.619890</td>
      <td>0.681958</td>
      <td>...</td>
      <td>0.868716</td>
      <td>-0.049282</td>
      <td>-1.055444</td>
      <td>-1.229350</td>
      <td>0.512756</td>
      <td>0.157752</td>
      <td>-0.553901</td>
      <td>1.588311</td>
      <td>0.999667</td>
      <td>0.447293</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.012128</td>
      <td>-1.367315</td>
      <td>0.634536</td>
      <td>0.302416</td>
      <td>-0.766014</td>
      <td>-0.291953</td>
      <td>1.203971</td>
      <td>0.587154</td>
      <td>0.958710</td>
      <td>-0.065375</td>
      <td>...</td>
      <td>-0.909839</td>
      <td>-0.816945</td>
      <td>-1.560845</td>
      <td>0.579045</td>
      <td>0.005077</td>
      <td>0.157752</td>
      <td>0.838587</td>
      <td>0.078715</td>
      <td>1.013894</td>
      <td>0.352525</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.224895</td>
      <td>-1.194018</td>
      <td>1.412153</td>
      <td>0.302416</td>
      <td>-0.981792</td>
      <td>1.769586</td>
      <td>1.203971</td>
      <td>-0.999748</td>
      <td>0.628121</td>
      <td>0.535437</td>
      <td>...</td>
      <td>1.513066</td>
      <td>-0.372056</td>
      <td>-1.560845</td>
      <td>-0.900752</td>
      <td>0.005077</td>
      <td>-0.518328</td>
      <td>0.219704</td>
      <td>0.725685</td>
      <td>0.831718</td>
      <td>-1.828897</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>340795</th>
      <td>1.200640</td>
      <td>0.712252</td>
      <td>0.167965</td>
      <td>-0.317925</td>
      <td>-1.197571</td>
      <td>-0.769214</td>
      <td>-1.278443</td>
      <td>1.539295</td>
      <td>0.958710</td>
      <td>-0.344894</td>
      <td>...</td>
      <td>0.909669</td>
      <td>1.288355</td>
      <td>-1.223911</td>
      <td>-0.251674</td>
      <td>1.020436</td>
      <td>-0.969049</td>
      <td>0.838587</td>
      <td>0.078715</td>
      <td>1.035668</td>
      <td>0.077653</td>
    </tr>
    <tr>
      <th>340796</th>
      <td>1.200640</td>
      <td>0.538954</td>
      <td>0.167965</td>
      <td>0.922758</td>
      <td>0.744436</td>
      <td>-1.120300</td>
      <td>1.203971</td>
      <td>-1.634509</td>
      <td>-1.024828</td>
      <td>-1.678996</td>
      <td>...</td>
      <td>0.649671</td>
      <td>0.835020</td>
      <td>1.303090</td>
      <td>1.078315</td>
      <td>0.512756</td>
      <td>0.833833</td>
      <td>1.302750</td>
      <td>-1.215225</td>
      <td>0.154496</td>
      <td>0.301785</td>
    </tr>
    <tr>
      <th>340797</th>
      <td>-0.012128</td>
      <td>1.058846</td>
      <td>0.790059</td>
      <td>-1.558608</td>
      <td>-0.981792</td>
      <td>1.320509</td>
      <td>-1.278443</td>
      <td>-1.634509</td>
      <td>0.628121</td>
      <td>0.141773</td>
      <td>...</td>
      <td>-0.219062</td>
      <td>0.936870</td>
      <td>0.629223</td>
      <td>-0.091423</td>
      <td>-1.517962</td>
      <td>1.059193</td>
      <td>0.529146</td>
      <td>-1.646538</td>
      <td>-0.543317</td>
      <td>0.626878</td>
    </tr>
    <tr>
      <th>340798</th>
      <td>-0.012128</td>
      <td>0.885549</td>
      <td>1.412153</td>
      <td>-0.938266</td>
      <td>1.175993</td>
      <td>-0.822883</td>
      <td>1.203971</td>
      <td>-0.999748</td>
      <td>0.297531</td>
      <td>-0.051350</td>
      <td>...</td>
      <td>-0.315874</td>
      <td>-1.397704</td>
      <td>-1.223911</td>
      <td>0.448527</td>
      <td>1.528116</td>
      <td>-0.518328</td>
      <td>-1.327506</td>
      <td>1.156998</td>
      <td>0.509574</td>
      <td>-0.429399</td>
    </tr>
    <tr>
      <th>340799</th>
      <td>-0.012128</td>
      <td>0.885549</td>
      <td>0.323489</td>
      <td>-1.558608</td>
      <td>-1.197571</td>
      <td>0.290188</td>
      <td>-1.278443</td>
      <td>0.269773</td>
      <td>0.628121</td>
      <td>-0.465884</td>
      <td>...</td>
      <td>-1.931259</td>
      <td>1.438801</td>
      <td>-0.044644</td>
      <td>-0.074317</td>
      <td>-1.010282</td>
      <td>-0.518328</td>
      <td>-1.327506</td>
      <td>0.725685</td>
      <td>1.391821</td>
      <td>-0.452211</td>
    </tr>
  </tbody>
</table>
<p>340800 rows × 30 columns</p>
</div>




```python
concat_fund_df = pd.concat([concat_fund_df.iloc[:, :1], concat_fund_df_scaled, concat_fund_df.iloc[:, 31:]], axis=1)
concat_fund_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>基金代码</th>
      <th>feature1</th>
      <th>feature2</th>
      <th>feature3</th>
      <th>feature4</th>
      <th>feature5</th>
      <th>feature6</th>
      <th>feature7</th>
      <th>feature8</th>
      <th>feature9</th>
      <th>...</th>
      <th>day_23</th>
      <th>day_24</th>
      <th>day_25</th>
      <th>day_26</th>
      <th>day_27</th>
      <th>day_28</th>
      <th>day_29</th>
      <th>day_30</th>
      <th>day_31</th>
      <th>净值</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>J0045</td>
      <td>1.200640</td>
      <td>0.712252</td>
      <td>-1.076222</td>
      <td>-1.248437</td>
      <td>-0.334457</td>
      <td>0.956536</td>
      <td>-1.278443</td>
      <td>-0.047607</td>
      <td>0.958710</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-4.897256487</td>
    </tr>
    <tr>
      <th>1</th>
      <td>J0109</td>
      <td>1.200640</td>
      <td>-0.500829</td>
      <td>-1.231746</td>
      <td>-0.938266</td>
      <td>-0.981792</td>
      <td>-0.654327</td>
      <td>-1.278443</td>
      <td>0.269773</td>
      <td>0.958710</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-4.547777652</td>
    </tr>
    <tr>
      <th>2</th>
      <td>J0063</td>
      <td>1.200640</td>
      <td>-0.327532</td>
      <td>-0.765175</td>
      <td>-0.317925</td>
      <td>-0.981792</td>
      <td>0.304544</td>
      <td>1.203971</td>
      <td>0.587154</td>
      <td>1.619890</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>-4.306791468</td>
    </tr>
    <tr>
      <th>3</th>
      <td>J0099</td>
      <td>-0.012128</td>
      <td>-1.367315</td>
      <td>0.634536</td>
      <td>0.302416</td>
      <td>-0.766014</td>
      <td>-0.291953</td>
      <td>1.203971</td>
      <td>0.587154</td>
      <td>0.958710</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>12.98199228</td>
    </tr>
    <tr>
      <th>4</th>
      <td>J0001</td>
      <td>-1.224895</td>
      <td>-1.194018</td>
      <td>1.412153</td>
      <td>0.302416</td>
      <td>-0.981792</td>
      <td>1.769586</td>
      <td>1.203971</td>
      <td>-0.999748</td>
      <td>0.628121</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-3.483643378</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>340795</th>
      <td>J0196</td>
      <td>1.200640</td>
      <td>0.712252</td>
      <td>0.167965</td>
      <td>-0.317925</td>
      <td>-1.197571</td>
      <td>-0.769214</td>
      <td>-1.278443</td>
      <td>1.539295</td>
      <td>0.958710</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>8.075655166</td>
    </tr>
    <tr>
      <th>340796</th>
      <td>J0197</td>
      <td>1.200640</td>
      <td>0.538954</td>
      <td>0.167965</td>
      <td>0.922758</td>
      <td>0.744436</td>
      <td>-1.120300</td>
      <td>1.203971</td>
      <td>-1.634509</td>
      <td>-1.024828</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>7.724697415</td>
    </tr>
    <tr>
      <th>340797</th>
      <td>J0198</td>
      <td>-0.012128</td>
      <td>1.058846</td>
      <td>0.790059</td>
      <td>-1.558608</td>
      <td>-0.981792</td>
      <td>1.320509</td>
      <td>-1.278443</td>
      <td>-1.634509</td>
      <td>0.628121</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-1.767100969</td>
    </tr>
    <tr>
      <th>340798</th>
      <td>J0199</td>
      <td>-0.012128</td>
      <td>0.885549</td>
      <td>1.412153</td>
      <td>-0.938266</td>
      <td>1.175993</td>
      <td>-0.822883</td>
      <td>1.203971</td>
      <td>-0.999748</td>
      <td>0.297531</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-2.661014584</td>
    </tr>
    <tr>
      <th>340799</th>
      <td>J0200</td>
      <td>-0.012128</td>
      <td>0.885549</td>
      <td>0.323489</td>
      <td>-1.558608</td>
      <td>-1.197571</td>
      <td>0.290188</td>
      <td>-1.278443</td>
      <td>0.269773</td>
      <td>0.628121</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4.231475045</td>
    </tr>
  </tbody>
</table>
<p>340800 rows × 138 columns</p>
</div>



## 3.合并客户数据和基金数据


```python
# 处理六：合并两张表格
df = pd.concat([new_customer_df, concat_fund_df], axis=1)
```


```python
# 删除列的标签，axis=1 表示列
df = df.drop(['选择基金', '基金代码', '基金成立时间', 'date'], axis=1)
# 将target'选择排序' 移到最后一列
df['选择排序'] = df.pop('选择排序')
# 修改 feature为 fund_feature
for i in range(40, 70):
    df = df.rename(columns={df.columns[i]: 'fund_' + df.columns[i]})
```


```python
# 输出csv
# df.to_csv('output.csv', encoding='utf-8')
```

## 4.预测


```python
from sklearn.model_selection import train_test_split

# 划分数据集为训练集和测试集
X = df.drop('选择排序', axis=1)
y = df['选择排序']

# 划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

```


```python
# Perceptron模型
from sklearn.linear_model import Perceptron
perceptron_classifier = Perceptron(random_state=42, verbose=True, max_iter=1000)

perceptron_classifier.fit(X_train, y_train)
y_pred_pc = perceptron_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_pc)
print(f"Accuracy using Perceptron: {accuracy}, y_pred is {y_pred_pc}")
```

    -- Epoch 1
    Norm: 92.46, NNZs: 167, Bias: 68.000000, T: 272640, Avg. loss: 1.252741
    Total training time: 0.09 seconds.
    -- Epoch 2
    Norm: 106.06, NNZs: 164, Bias: 68.000000, T: 545280, Avg. loss: 1.228747
    Total training time: 0.25 seconds.
    -- Epoch 3
    Norm: 121.13, NNZs: 166, Bias: 67.000000, T: 817920, Avg. loss: 1.230478
    Total training time: 0.37 seconds.
    -- Epoch 4
    Norm: 126.41, NNZs: 165, Bias: 63.000000, T: 1090560, Avg. loss: 1.230669
    Total training time: 0.53 seconds.
    -- Epoch 5
    Norm: 120.86, NNZs: 167, Bias: 62.000000, T: 1363200, Avg. loss: 1.225637
    Total training time: 0.68 seconds.
    -- Epoch 6
    Norm: 128.14, NNZs: 170, Bias: 69.000000, T: 1635840, Avg. loss: 1.224242
    Total training time: 0.79 seconds.
    -- Epoch 7
    Norm: 125.84, NNZs: 167, Bias: 61.000000, T: 1908480, Avg. loss: 1.222632
    Total training time: 0.89 seconds.
    -- Epoch 8
    Norm: 123.26, NNZs: 163, Bias: 69.000000, T: 2181120, Avg. loss: 1.218295
    Total training time: 1.00 seconds.
    -- Epoch 9
    Norm: 126.76, NNZs: 167, Bias: 64.000000, T: 2453760, Avg. loss: 1.227991
    Total training time: 1.10 seconds.
    -- Epoch 10
    Norm: 131.06, NNZs: 169, Bias: 65.000000, T: 2726400, Avg. loss: 1.234568
    Total training time: 1.23 seconds.
    -- Epoch 11
    Norm: 132.18, NNZs: 166, Bias: 65.000000, T: 2999040, Avg. loss: 1.217471
    Total training time: 1.33 seconds.
    -- Epoch 12
    Norm: 131.57, NNZs: 165, Bias: 63.000000, T: 3271680, Avg. loss: 1.235366
    Total training time: 1.44 seconds.
    -- Epoch 13
    Norm: 136.27, NNZs: 166, Bias: 63.000000, T: 3544320, Avg. loss: 1.225213
    Total training time: 1.55 seconds.
    Convergence after 13 epochs took 1.55 seconds
    -- Epoch 1
    Norm: 78.94, NNZs: 162, Bias: -60.000000, T: 272640, Avg. loss: 0.565198
    Total training time: 0.13 seconds.
    -- Epoch 2
    Norm: 93.65, NNZs: 167, Bias: -60.000000, T: 545280, Avg. loss: 0.548994
    Total training time: 0.28 seconds.
    -- Epoch 3
    Norm: 107.45, NNZs: 167, Bias: -63.000000, T: 817920, Avg. loss: 0.543448
    Total training time: 0.38 seconds.
    -- Epoch 4
    Norm: 112.42, NNZs: 162, Bias: -66.000000, T: 1090560, Avg. loss: 0.542047
    Total training time: 0.49 seconds.
    -- Epoch 5
    Norm: 115.04, NNZs: 164, Bias: -62.000000, T: 1363200, Avg. loss: 0.550648
    Total training time: 0.60 seconds.
    -- Epoch 6
    Norm: 128.83, NNZs: 165, Bias: -66.000000, T: 1635840, Avg. loss: 0.540281
    Total training time: 0.71 seconds.
    -- Epoch 7
    Norm: 132.78, NNZs: 169, Bias: -63.000000, T: 1908480, Avg. loss: 0.542232
    Total training time: 0.82 seconds.
    -- Epoch 8
    Norm: 136.39, NNZs: 167, Bias: -64.000000, T: 2181120, Avg. loss: 0.546548
    Total training time: 0.92 seconds.
    -- Epoch 9
    Norm: 137.78, NNZs: 166, Bias: -70.000000, T: 2453760, Avg. loss: 0.539798
    Total training time: 1.04 seconds.
    -- Epoch 10
    Norm: 143.50, NNZs: 166, Bias: -69.000000, T: 2726400, Avg. loss: 0.544055
    Total training time: 1.27 seconds.
    -- Epoch 11
    Norm: 143.70, NNZs: 167, Bias: -64.000000, T: 2999040, Avg. loss: 0.543328
    Total training time: 1.36 seconds.
    Convergence after 11 epochs took 1.36 seconds
    -- Epoch 1
    Norm: 87.99, NNZs: 162, Bias: -65.000000, T: 272640, Avg. loss: 0.567236
    Total training time: 0.12 seconds.
    -- Epoch 2
    Norm: 93.60, NNZs: 166, Bias: -65.000000, T: 545280, Avg. loss: 0.552740
    Total training time: 0.23 seconds.
    -- Epoch 3
    Norm: 105.11, NNZs: 162, Bias: -63.000000, T: 817920, Avg. loss: 0.552061
    Total training time: 0.32 seconds.
    -- Epoch 4
    Norm: 110.40, NNZs: 164, Bias: -73.000000, T: 1090560, Avg. loss: 0.544508
    Total training time: 0.48 seconds.
    -- Epoch 5
    Norm: 111.62, NNZs: 167, Bias: -67.000000, T: 1363200, Avg. loss: 0.551475
    Total training time: 0.59 seconds.
    -- Epoch 6
    Norm: 112.71, NNZs: 169, Bias: -65.000000, T: 1635840, Avg. loss: 0.544028
    Total training time: 0.68 seconds.
    -- Epoch 7
    Norm: 117.17, NNZs: 169, Bias: -64.000000, T: 1908480, Avg. loss: 0.547210
    Total training time: 0.82 seconds.
    -- Epoch 8
    Norm: 117.80, NNZs: 168, Bias: -63.000000, T: 2181120, Avg. loss: 0.546045
    Total training time: 0.92 seconds.
    -- Epoch 9
    Norm: 125.14, NNZs: 170, Bias: -69.000000, T: 2453760, Avg. loss: 0.545288
    Total training time: 1.05 seconds.
    Convergence after 9 epochs took 1.05 seconds
    -- Epoch 1
    Norm: 69.14, NNZs: 156, Bias: -65.000000, T: 272640, Avg. loss: 0.151736
    Total training time: 0.12 seconds.
    -- Epoch 2
    Norm: 79.10, NNZs: 162, Bias: -72.000000, T: 545280, Avg. loss: 0.138197
    Total training time: 0.26 seconds.
    -- Epoch 3
    Norm: 79.31, NNZs: 164, Bias: -70.000000, T: 817920, Avg. loss: 0.140124
    Total training time: 0.37 seconds.
    -- Epoch 4
    Norm: 82.79, NNZs: 165, Bias: -68.000000, T: 1090560, Avg. loss: 0.137800
    Total training time: 0.49 seconds.
    -- Epoch 5
    Norm: 90.31, NNZs: 166, Bias: -68.000000, T: 1363200, Avg. loss: 0.137556
    Total training time: 0.59 seconds.
    -- Epoch 6
    Norm: 89.96, NNZs: 163, Bias: -69.000000, T: 1635840, Avg. loss: 0.137137
    Total training time: 0.71 seconds.
    -- Epoch 7
    Norm: 94.54, NNZs: 165, Bias: -66.000000, T: 1908480, Avg. loss: 0.139736
    Total training time: 0.83 seconds.
    Convergence after 7 epochs took 0.83 seconds
    -- Epoch 1
    Norm: 61.09, NNZs: 148, Bias: -69.000000, T: 272640, Avg. loss: 0.029810
    Total training time: 0.14 seconds.
    -- Epoch 2
    Norm: 69.45, NNZs: 148, Bias: -70.000000, T: 545280, Avg. loss: 0.028391
    Total training time: 0.25 seconds.
    -- Epoch 3
    Norm: 74.09, NNZs: 157, Bias: -71.000000, T: 817920, Avg. loss: 0.025827
    Total training time: 0.38 seconds.
    -- Epoch 4
    Norm: 78.66, NNZs: 162, Bias: -71.000000, T: 1090560, Avg. loss: 0.025156
    Total training time: 0.50 seconds.
    -- Epoch 5
    Norm: 85.40, NNZs: 163, Bias: -71.000000, T: 1363200, Avg. loss: 0.024645
    Total training time: 0.61 seconds.
    -- Epoch 6
    Norm: 86.81, NNZs: 167, Bias: -71.000000, T: 1635840, Avg. loss: 0.024701
    Total training time: 0.74 seconds.
    -- Epoch 7
    Norm: 93.31, NNZs: 164, Bias: -74.000000, T: 1908480, Avg. loss: 0.023552
    Total training time: 0.85 seconds.
    -- Epoch 8
    Norm: 94.23, NNZs: 166, Bias: -70.000000, T: 2181120, Avg. loss: 0.025229
    Total training time: 0.96 seconds.
    -- Epoch 9
    Norm: 95.97, NNZs: 166, Bias: -74.000000, T: 2453760, Avg. loss: 0.023290
    Total training time: 1.08 seconds.
    -- Epoch 10
    Norm: 100.04, NNZs: 166, Bias: -74.000000, T: 2726400, Avg. loss: 0.023572
    Total training time: 1.18 seconds.
    -- Epoch 11
    Norm: 102.60, NNZs: 163, Bias: -74.000000, T: 2999040, Avg. loss: 0.023608
    Total training time: 1.31 seconds.
    -- Epoch 12
    Norm: 104.93, NNZs: 165, Bias: -74.000000, T: 3271680, Avg. loss: 0.023670
    Total training time: 1.47 seconds.
    Convergence after 12 epochs took 1.47 seconds
    Accuracy using Perceptron: 0.9211707746478873, y_pred is [0 0 0 ... 0 2 0]



```python
# logistic regression逻辑回归模型
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(multi_class='auto', solver='liblinear', max_iter=1000, verbose=1)
logreg.fit(X_train, y_train)

y_pred_lr = logreg.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_lr)
print(f"Accuracy using logistic regression: {accuracy}, y_pred is {y_pred_lr}")
```

    [LibLinear]Accuracy using logistic regression: 0.9883656103286385, y_pred is [0 0 0 ... 0 0 0]



```python
# RandomForest随机森林
from sklearn.ensemble import RandomForestClassifier

rf_classifier = RandomForestClassifier(random_state=42, n_estimators=500, n_jobs = -1, verbose=1)
rf_classifier.fit(X_train, y_train)

y_pred_rf = rf_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy using RandomForest: {accuracy}, y_pred is {y_pred_rf}")
```

    [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.
    [Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   10.3s
    [Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.1min
    [Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  2.6min
    [Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.0min finished
    [Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.
    [Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s
    [Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.9s
    [Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    2.2s


    Accuracy using RandomForest: 0.9883656103286385, y_pred is [0 0 0 ... 0 0 0]


    [Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:    2.6s finished



```python
# # SVM分类
# from sklearn.svm import SVC

# svm_classifier = SVC(decision_function_shape='ovr', random_state=42)
# svm_classifier.fit(X_train, y_train)

# y_pred = svm_classifier.predict(X_test)

# accuracy = accuracy_score(y_test, y_pred)
# print(f"Accuracy: {accuracy}")
```
